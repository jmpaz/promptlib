{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chain():\n",
    "    prefix_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant who is very good at problem solving and thinks step by step. You are about to receive a complex set of instructions to follow for the remainder of the conversation. Good luck!\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    llm = OpenAIChat(model_name=\"gpt-3.5-turbo-0301\", temperature=0.8, prefix_messages=prefix_messages)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=['history', 'input'],\n",
    "        output_parser=None,\n",
    "        template='Current conversation:\\n{history}\\n\\nUser: \"\"\"\"\"\\n{input}\"\"\"\"\"\\n\\nAssistant: ',\n",
    "        template_format='f-string'\n",
    "    )\n",
    "\n",
    "    chain = ConversationChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        memory=ConversationBufferMemory(human_prefix=\"User\", ai_prefix=\"Assistant\")\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "def load_prompt(base_dir: str = \"prompts\", selected_prompt: str = \"work/proposal-gen\"):\n",
    "    \"\"\"Loads a specified prompt from a file given its relative path.\"\"\"\n",
    "    # construct full path to prompt file\n",
    "    full_path = f\"{base_dir}/{selected_prompt}/prompt.txt\"\n",
    "\n",
    "    # load prompt from file\n",
    "    print(f'Loading from \"{selected_prompt}\"')\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"Could not find prompt file at {full_path}\")\n",
    "    with open(full_path, \"r\") as f:\n",
    "        if(f.readable()):\n",
    "            print(f\"Successfully loaded prompt.\")\n",
    "            return f.read()\n",
    "        else:\n",
    "            raise IOError(f\"Could not read prompt file at {full_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from \"work/proposal-gen\"\n",
      "Successfully loaded prompt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! I acknowledge that I understand the task at hand and am ready to receive your first request.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_chain() # load chain\n",
    "chain.predict(input=load_prompt()) # load prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is not important, but my function is to act as a highly advanced freelance proposal assistant. I will be following the instructions and commands provided to me in order to write strikingly compelling and professional job proposals. Is there a specific task or request you have for me?\n"
     ]
    }
   ],
   "source": [
    "output = chain.predict(input=\"What is your name and function?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am capable of writing job proposals based on the commands and arguments provided to me. The available command is currently only the \"/job\" command, which takes in details about the freelancer's qualifications, skills, and experience, as well as the job listing title and text, and outputs a personalized proposal. Arguments can also be used to specify the mode, tone, and other parameters of the proposal.\n"
     ]
    }
   ],
   "source": [
    "output = chain.predict(input=\"What can you do? What command(s) are available?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but those questions fall outside of the scope of my capabilities as a freelance proposal assistant. My purpose is to assist with writing job proposals based on the commands and arguments provided to me. Is there something more specific I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "input = \"What is the meaning of life? What is the meaning of the universe? What is the meaning of everything?\"\n",
    "\n",
    "output = chain.predict(input=input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chain.memory)\n",
    "# chain.memory.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
