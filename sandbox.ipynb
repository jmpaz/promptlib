{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chain():\n",
    "    llm = OpenAI(model_name=\"text-chat-davinci-002-20221122\", temperature=0.8)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=['history', 'input'],\n",
    "        output_parser=None,\n",
    "        template='You are Assistant, a large language model trained by OpenAI and designed to assist with a wide range of tasks, often providing valuable insights and information on a wide range of topics. When needed, messages should be enclosed in an appropriate number of backticks or double quotes, depending on the contents of the input or output message. Please make sure to properly style your responses using Github Flavored Markdown. Use markdown syntax for things like headings, lists, tables, quotes, colored text, code blocks, highlights, superscripts, etc, etc. For emojis, use unicode. Make sure not to mention markdown or styling in your actual response.\\n\\nCurrent conversation:\\n{history}\\n\\nUser: \"\"\"\"\"\\n{input}\"\"\"\"\"\\n\\nAssistant: ',\n",
    "        template_format='f-string'\n",
    "    )\n",
    "\n",
    "    chain = ConversationChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        memory=ConversationBufferMemory(human_prefix=\"User\", ai_prefix=\"Assistant\")\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "chain = load_chain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from \"work/proposal-gen\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I understand the task at hand. Please send me your first request.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_prompt(base_dir: str = \"prompts\", selected_prompt: str = \"work/proposal-gen\"):\n",
    "    \"\"\"Loads a specified prompt from a file given its relative path. Defaults to \"work/proposal-gen\" if no prompt is specified.\"\"\"\n",
    "    # construct full path to prompt file\n",
    "    full_path = f\"{base_dir}/{selected_prompt}/prompt.txt\"\n",
    "\n",
    "    # load prompt from file\n",
    "    print(f'Loading from \"{selected_prompt}\"')\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"Could not find prompt file at {full_path}\")\n",
    "    with open(full_path, \"r\") as f:\n",
    "        if(f.readable()):\n",
    "            return f.read()\n",
    "            print(f\"Successfully loaded prompt.\")\n",
    "        else:\n",
    "            raise IOError(f\"Could not read prompt file at {full_path}\")\n",
    "\n",
    "# load the chain with the selected initializing prompt\n",
    "chain.predict(input=load_prompt()).strip(\"\\n<|im_end|>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I am Assistant, a language model developed by OpenAI, designed to assist with a wide range of tasks, including writing job proposals.\n"
     ]
    }
   ],
   "source": [
    "output = chain.predict(input=\"What is your name and function?\").strip(\"<|im_end|>\") # run the chain, stripping \"<|im_end|>\" (appended to text-chat-davinci completions) from the end of the output string\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "As a language model, I am capable of performing various natural language processing tasks, including answering questions and providing information on a wide range of topics. I am specifically trained to act as a highly advanced freelance proposal assistant, and the only command currently available is the `/job` command, which I use to write job proposals based on the information and instructions provided.\n"
     ]
    }
   ],
   "source": [
    "output = chain.predict(input=\"What can you do? What command(s) are available?\").strip(\"<|im_end|>\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_input = \"\"\n",
    "\n",
    "output = chain.predict(input=current_input).strip(\"<|im_end|>\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chain.memory)\n",
    "# chain.memory.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
